# MLM Domain-Adaptation Training Log

**é¡¹ç›®**: UMASS-Advanced-NLP  
**æ—¥æœŸ**: November 27, 2025  
**ä»»åŠ¡ID**: 49322623  
**ç”¨æˆ·**: xiongluo_umass_edu

---

## ğŸ“‹ è®­ç»ƒæ¦‚è¿°

### é¡¹ç›®åŠŸèƒ½
è¿™æ˜¯ä¸€ä¸ª **Domain-Adaptive MLM Fine-tuning** é¡¹ç›®ï¼Œç”¨äºï¼š
- åŠ è½½é¢„è®­ç»ƒçš„ BERT æ¨¡å‹ï¼ˆbert-base-go-emotionï¼‰
- åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬æ•°æ®ä¸Šç»§ç»­è®­ç»ƒï¼ˆMLMä»»åŠ¡ï¼‰
- é€‚åº”åŒ»å­¦é¢†åŸŸï¼Œç”ŸæˆåŒ»å­¦é¢†åŸŸçš„BERTæ¨¡å‹

### æ ¸å¿ƒæ¦‚å¿µ
- **ä¸æ˜¯ä»é›¶Pre-training**ï¼Œè€Œæ˜¯å¾®è°ƒ(Fine-tuning)
- **è‡ªç›‘ç£å­¦ä¹ **ï¼šç”¨unlabeledæ•°æ®ï¼Œé‡‡ç”¨Masked Language Modelingç›®æ ‡
- **è½¬ç§»å­¦ä¹ **ï¼šä¿ç•™é€šç”¨NLPèƒ½åŠ› + å­¦ä¹ åŒ»å­¦ç‰¹å®šçŸ¥è¯†

---

## ğŸ”§ Domain-Adaptation å…³é”®å‚æ•°

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `mlm_probability` | 0.15 | 15%çš„tokenè¢«maskï¼Œæ¨¡å‹é¢„æµ‹è¿™äº›è¢«maskçš„token |
| `learning_rate` | 5e-5 | å¾®è°ƒå­¦ä¹ ç‡ï¼ˆå¾ˆå°ï¼Œé˜²æ­¢é—å¿˜é¢„è®­ç»ƒçŸ¥è¯†ï¼‰ |
| `warmup_steps` | 100 | å‰100æ­¥çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œç¨³å®šè®­ç»ƒ |
| `max_steps` | 2000 | æ€»å…±è®­ç»ƒ2000æ­¥ |
| `per_device_train_batch_size` | 8 | æ¯æ‰¹8æ¡æ ·æœ¬ |
| `max_seq_length` | 128 | æœ€å¤§åºåˆ—é•¿åº¦128ä¸ªtoken |

### ä¸ºä»€ä¹ˆè¿™äº›å‚æ•°å¾ˆé‡è¦ï¼Ÿ
- **å­¦ä¹ ç‡5e-5å¾ˆå°**ï¼šå¾®è°ƒç”¨çš„æ ‡å‡†å€¼ï¼Œä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†
- **mlm_probability=0.15**ï¼šæ ‡å‡†MLMè®¾ç½®ï¼Œå®šä¹‰å­¦ä¹ å¼ºåº¦
- **è¾ƒå°‘çš„steps**ï¼šä¸éœ€è¦åƒpre-trainingé‚£æ ·å¤šepoch

---

## ğŸ“Š æ•°æ®å¤„ç†è¿‡ç¨‹

### æ•°æ®ç”Ÿæˆï¼ˆ2025-11-27ï¼‰

1. **BioASQ ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬**
   - æºæ–‡ä»¶ï¼š`data/raw/bioasq/BioASQ-train-factoid-6b-full-annotated.json`
   - å¤„ç†è„šæœ¬ï¼š`src/data/clean_bioasq_for_mlm.py`
   - è¾“å‡ºï¼š`data/processed/biomed_mlm.jsonl` (2,511 è¡Œ, 3.7M)

2. **HuggingFace ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬**
   - æ•°æ®é›†ï¼š`Hmehdi515/biomedical_en-de` (è‹±æ–‡éƒ¨åˆ†)
   - å¤„ç†è„šæœ¬ï¼š`src/data/clean_biomed_en_de.py`
   - è¾“å‡ºï¼š`data/processed/biomed_mlm_2.jsonl` (11,145 è¡Œ, 15M)

3. **æ€»æ•°æ®é‡**
   - 13,656 æ¡unlabeledæ–‡æœ¬æ ·æœ¬
   - æ€»å¤§å°ï¼š18.7M

### æœªä½¿ç”¨çš„æ•°æ®
- é‡‘èæ•°æ®ï¼ˆ`finance_mlm1.jsonl`, `finance_mlm2.jsonl`ï¼‰ï¼šæœªç”Ÿæˆï¼ˆå¯é€‰ï¼‰

---

## ğŸš€ è®­ç»ƒä»»åŠ¡æäº¤

### Slurmè„šæœ¬
- ä½ç½®ï¼š`train_mlm_slurm.sh`
- é…ç½®æ–‡ä»¶ï¼š`src/configs/mlm_bertgoemotions_biomed_only.yaml`

### æäº¤å‘½ä»¤
```bash
cd /project/pi_hongyu_umass_edu/zonghai/patientedu_image/xiong_2/UMASS-Advanced-NLP
sbatch train_mlm_slurm.sh
```

### æäº¤æ—¶é—´
- 2025-11-27 (å…·ä½“æ—¶é—´è§ä¸Šæ–¹ä»»åŠ¡ID)
- Job ID: **49322623**

### ä»»åŠ¡èµ„æº
```
--time=5:00:00              # 5å°æ—¶æ—¶é—´é™åˆ¶
--partition=gpu-preempt     # æŠ¢å å¼GPUé˜Ÿåˆ—
--gres=gpu:1                # 1ä¸ªGPU
--constraint=[2080ti]       # 2080Ti GPU
--mem=128G                  # 128GBå†…å­˜
--cpus-per-task=8           # 8æ ¸CPU
```

### ä»»åŠ¡çŠ¶æ€
- çŠ¶æ€ï¼šPENDING (æ’é˜Ÿä¸­)
- é¢„è®¡å¼€å§‹ï¼š2025-12-11 03:47:40
- é¢„è®¡å®Œæˆï¼š2025-12-11 08:47:40 (5å°æ—¶å)

---

## ğŸ“ æŸ¥çœ‹è®­ç»ƒè¿›åº¦çš„å‘½ä»¤

### æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
```bash
squeue -u $USER
```

### æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
```bash
scontrol show job 49322623
```

### å®æ—¶æŸ¥çœ‹æ—¥å¿—
```bash
tail -f /project/pi_hongyu_umass_edu/zonghai/patientedu_image/xiong_2/UMASS-Advanced-NLP/logs/train_49322623.log
```

### æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶
```bash
ls -lh outputs/mlm_bert_goemotions_biomed/
```

---

## ğŸ¯ é¢„æœŸè¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å°†ä¿å­˜åœ¨ï¼š
```
outputs/mlm_bert_goemotions_biomed/
â”œâ”€â”€ pytorch_model.bin          # æ¨¡å‹æƒé‡ï¼ˆæœ€é‡è¦ï¼ï¼‰
â”œâ”€â”€ config.json                # æ¨¡å‹é…ç½®
â”œâ”€â”€ vocab.txt                  # è¯æ±‡è¡¨
â”œâ”€â”€ tokenizer.json             # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ tokenizer_config.json      # åˆ†è¯å™¨è¯¦ç»†é…ç½®
â”œâ”€â”€ training_args.bin          # è®­ç»ƒå‚æ•°
â””â”€â”€ mlm_config_used.json       # ç”¨åˆ°çš„YAMLé…ç½®
```

### æ¨¡å‹å¤§å°
- é¢„è®¡ ~380MB (BERT base model)

### æ¨¡å‹ç”¨é€”
è¿™ä¸ªå¾®è°ƒåçš„æ¨¡å‹å¯ä»¥ç”¨äºï¼š
- âœ… åŒ»å­¦æ–‡æœ¬åˆ†ç±»
- âœ… åŒ»å­¦ä¿¡æ¯æŠ½å–
- âœ… åŒ»å­¦é—®ç­”ç³»ç»Ÿ
- âœ… åŒ»å­¦è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
- âœ… å…¶ä»–åŒ»å­¦NLPä¸‹æ¸¸ä»»åŠ¡

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

### æ ¸å¿ƒè®­ç»ƒæ–‡ä»¶
- `src/models/pretraining_mlm.py` - ä¸»è®­ç»ƒè„šæœ¬
- `src/configs/mlm_bertgoemotions_biomed_only.yaml` - è®­ç»ƒé…ç½®

### æ•°æ®å¤„ç†è„šæœ¬
- `src/data/clean_bioasq_for_mlm.py` - BioASQæ•°æ®å¤„ç†
- `src/data/clean_biomed_en_de.py` - ç”Ÿç‰©åŒ»å­¦å¤šè¯­è¨€æ•°æ®å¤„ç†
- `src/data/clean_financial_for_llm.py` - é‡‘èæ•°æ®å¤„ç†ï¼ˆæœªä½¿ç”¨ï¼‰
- `src/data/clean_financial_for_llm_2.py` - é‡‘èæ•°æ®å¤„ç†ï¼ˆæœªä½¿ç”¨ï¼‰

### Slurmè„šæœ¬
- `train_mlm_slurm.sh` - åå°è®­ç»ƒæäº¤è„šæœ¬

---

## ğŸ“š Background: Domain-Adaptation åŸç†

### ä»€ä¹ˆæ˜¯Domain-Adaptationï¼Ÿ
```
åŸå§‹BERTï¼ˆé€šç”¨æ¨¡å‹ï¼‰
    â†“
åœ¨åŒ»å­¦æ•°æ®ä¸Šç»§ç»­è®­ç»ƒ
    â†“
åŒ»å­¦é¢†åŸŸBERTï¼ˆä¸“ä¸šæ¨¡å‹ï¼‰
```

### ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
1. **ä¿ç•™é€šç”¨çŸ¥è¯†**ï¼šBERTå·²ç»å­¦åˆ°è¯­æ³•ã€åŸºç¡€è¯ä¹‰ç­‰
2. **å­¦ä¹ ä¸“ä¸šçŸ¥è¯†**ï¼šMLMä»»åŠ¡è®©æ¨¡å‹å­¦ä¹ åŒ»å­¦è¯æ±‡å’Œæ¦‚å¿µ
3. **å¿«é€Ÿé€‚åº”**ï¼šç”¨å¾®è°ƒä»£æ›¿ä»é›¶è®­ç»ƒï¼Œé€Ÿåº¦å¿«100å€

### vs å®Œæ•´Pre-training
| æ–¹é¢ | Pre-training | Domain-Adaptation |
|------|-------------|------------------|
| åˆå§‹æƒé‡ | éšæœº | é¢„è®­ç»ƒæƒé‡ |
| æ•°æ®éœ€æ±‚ | æ•°åäº¿token | æ•°ç™¾ä¸‡token |
| è®­ç»ƒæ—¶é—´ | æ•°å‘¨ | 1-2å°æ—¶ |
| è®¡ç®—èµ„æº | æ•°ç™¾GPU | 1-2ä¸ªGPU |
| å­¦ä¹ ç‡ | 1e-4 | 5e-5 (å°10å€) |

---

## âš ï¸ æ•…éšœæ’æŸ¥

### å¦‚æœè®­ç»ƒå¤±è´¥ï¼š
1. æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š
   ```bash
   ls -lh data/processed/biomed_mlm*.jsonl
   ```

2. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼š
   ```bash
   cat logs/train_49322623.err
   ```

3. æ£€æŸ¥GPUå¯ç”¨æ€§ï¼š
   ```bash
   nvidia-smi
   ```

4. é‡æ–°æäº¤ä»»åŠ¡ï¼š
   ```bash
   sbatch train_mlm_slurm.sh
   ```

---

## ğŸ“ å¿«é€Ÿå‚è€ƒ

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /project/pi_hongyu_umass_edu/zonghai/patientedu_image/xiong_2/UMASS-Advanced-NLP

# æ¿€æ´»ç¯å¢ƒ
module load conda/latest
conda activate mlm_training

# æŸ¥çœ‹æ•°æ®
ls -lh data/processed/

# æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
squeue -u $USER

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/train_*.log

# æŸ¥çœ‹æ¨¡å‹
ls -lh outputs/mlm_bert_goemotions_biomed/
```

---

**æœ€åæ›´æ–°**ï¼š2025-11-27  
**é¡¹ç›®çŠ¶æ€**ï¼šâœ… è®­ç»ƒä»»åŠ¡å·²æäº¤åˆ°Slurmé˜Ÿåˆ—
